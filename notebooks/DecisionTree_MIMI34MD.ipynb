{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting in-hospital mortality\n",
    "\n",
    "As we discussed in our session yesterday, we want to be able to predict in-hospital mortality since it helps us communicate bette the risks and benefits to patients, families, and caregivers. For this exercise we will be using the same dataset we used for the exploratory data analysis excercisr from a couple of days ago.\n",
    "\n",
    "As always, we start by importing the packages that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to read the data. It is divided into two csv files, one with the patient attributes and the other one with the outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/home/shared/icu_2012.txt\")\n",
    "outcomes = pd.read_csv(\"/home/shared/Outcomes-a.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `In-hospital_death` label. You can see that it also has other variables like Length of stay and severity scores (SAPS-I, SOFA) which now you know what they are.\n",
    "\n",
    "For practical reasons, I want to merge these two datasets so we will do a left join. This means that we wil 'stick' the outcomes labels and add them to the dataset with patient attributes. That is what the `pd.merge` method does, it matches according to `Record_ID`.\n",
    "\n",
    "I'll also change the name of `In-hospital_death`, I need to get rid of the dash so I'll change it for an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = pd.merge(dataset, outcomes, on='RecordID', how='left')\n",
    "tree_dataset.rename(columns={'In-hospital_death': 'In_hospital_death'}, inplace=True)\n",
    "tree_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yo can scroll right and see that the columns have been added. Just like we did with our exploratory data analysis, we will take a look at our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a bit easier for us we will work with a subset of the variables. Here I arbitrarily decided to use the ones that make up the SOFA score. Usually this process of selecting features should be guided by clinical knowledge and the data that you are working with. I will go ahead and change my `tree_dataset` so it only has the columns that I want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = tree_dataset[['Age', 'Bilirubin_max', 'Creatinine_max', 'FiO2_max', 'GCS_min', 'MAP_min', 'Platelets_min', 'In_hospital_death']]\n",
    "tree_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier that we will be training doesn't like categorical variables not missing values. That is something that you need to work with. In this case we don't have any categorical variables so that is not a problem. If that were the case, we could use [`pd.get_dummies()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html).\n",
    "\n",
    "We do have quite some missing values. One thing you can do is impute missing data. Statisticians devote entire lifetimes of research to find better ways to [impute missing data](https://en.wikipedia.org/wiki/Imputation_%28statistics%29) (we will not do that here). I will fill the missing values with the last valid value in that column using the method `fillna` with the `backfill` option. There are many other (probably better) ways of doing this, this is just an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.fillna(method='backfill', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check if there are still missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilirrubin still has missing value because thhe `backfill` method only checks a number of columns down. I will go afead and replace the missing values with normal bilirrubins (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset[['Bilirubin_max']] = tree_dataset[['Bilirubin_max']].fillna(value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Now that we are ready, we will train our decision tree. But first we need to make sure we know where we are storing the patient data and the labels (in_hospital_death). We will create two new datasets:\n",
    "\n",
    "X will store all the patient variables\n",
    "y will store all the true labels\n",
    "\n",
    "Of course we could have worked with the two separate datasets that we had at the beginning!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am dropping the label, so X will have all the patient data \n",
    "X = tree_dataset.drop('In_hospital_death', axis=1)\n",
    "# Here I am setting y as only the labels\n",
    "y = tree_dataset['In_hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will divide our data in two. One sample of patients will be the **TRAINING SET**, the data I will use to train the model, the other sample will be the **TESTING SET** the data I will use to test gow good (or bad) is my model.\n",
    "\n",
    "To do this, I will import the necessary packages from scikit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what I am doing is the following:\n",
    "\n",
    "`X_train`: contains all patient attributes from the patients I'll use to train the model.\n",
    "\n",
    "`X_test`: contains all patient attributes from the patients I'll use to test the model. The model will not see this patients when I train it.\n",
    "\n",
    "Same thing for `y_train` and `y_test`\n",
    "\n",
    "And in the parameters, I am giving the function three elements: X (where all may patient data lives), y (where all the labels live), and `test_size` which defines the size of the training/testing sets. In this case is 80% training, 20% testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that  `X_test` has 800 patients (20% of the original 4000) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now import the packages we need to actually train the classifier. In this case we need a DecisionTreeClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the package\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# we create an instance of a DecisionTreeClassifier that we will name, creatively, classifier:\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# now we ask the classifier to learn from the training dataset:\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we can see that we didn't sepcify any specific variable for the decision tree so it used all the defaults. We will change some of that later. You can look at all the options [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to check how we did. For that, we will ask the classifier to predict the labels, given a set of patient attributes. We use the `predict` method and we pass it the `X_test` data that contains all the patient attributes that the classifier has not seen. We will store the predictions in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mindblowing!!! not really, we don't see anything happen. \n",
    "\n",
    "We now need to calculate some performance metrics. To do this we will use a confusion matrix and a classification report: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import what we need\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the classifier's predictions stored in `y_pred` with the truth that is stored in `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:20%\">\n",
    "<tr>\n",
    "<th></th>\n",
    "<th>Survival Pred</th>\n",
    "<th>Death Pred</th>\n",
    "<th>TOTAL</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>True Survival</td>\n",
    "<td>581</td>\n",
    "<td>104</td>\n",
    "<td>685</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>True Death</td>\n",
    "<td>77</td>\n",
    "<td>38</td>\n",
    "<td>115</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "tree_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Age', 'Bilirubin_max', 'Creatinine_max', 'FiO2_max', 'GCS_min',\n",
    "       'MAP_min', 'Platelets_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things about decision trees is that they are interpretable. I can understand how did are they built and what is guiding the decisions. Here we will visualize the decision tree we just built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(classifier, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very interpretable!! Since we did not set any option, it defaulted to creating a tree with unlimited depth, and that is what we got. Let's try 'pruning' the tree with a maximum depth of 6 so we can make a better interpretation, and let's see if we lose accuracy in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "classifier2 = DecisionTreeClassifier(max_depth = 6)\n",
    "classifier2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Our accuracy even went up a bit (from 77 to 84%). Let's see how the tree looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(classifier2, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better, but still not very intepretable. Let's try with Max depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = DecisionTreeClassifier(max_depth = 3)\n",
    "classifier3.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = classifier3.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Our accuracy is pretty much the same but now we might have a model that we can interpret better. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(classifier3, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can really interpret what the tree is doing to classify patients. However, we can see that is only predicting Death(+) in patients with ALL the following conditions: Creatinine > 1.35, Platelets < 86,500 and Bilirrubin >5.6.\n",
    "\n",
    "That is pretty narrow. It is getting away with reasonable accuracy because mortality in our dataset is low, also know as **Class Imbalance**. Since only 13% of all patients die, if the model predicts **survival** for all patients, it will be right 87% of the time... which is what is happening here. We will deal with that a bit later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset.In_hospital_death.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = classifier3.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = [p[1] for p in prob_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print( roc_auc_score(y, prob_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y, prob_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.0,1.0])\n",
    "plt.ylim([-0.0,1.0])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will try to fix the unbalanced class problem\n",
    "\n",
    "To acomplish this we will separate the dataset into those who died and those who survived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = tree_dataset[tree_dataset.In_hospital_death==0]\n",
    "df_minority = tree_dataset[tree_dataset.In_hospital_death==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see how it looks\n",
    "df_majority.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now the minority\n",
    "df_minority.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now upsample the death cases using the Pandas `resample()` method. Just as in imputation, there are MANY ways of doing this and some are smarter than this one, scikit has very smart methods of doing this too. We will not spend too much time doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=3446,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.In_hospital_death.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_upsampled.In_hospital_death\n",
    "X = df_upsampled.drop('In_hospital_death', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier4 = DecisionTreeClassifier(max_depth = 3)\n",
    "classifier4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = classifier4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred4))\n",
    "print(classification_report(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "dot_data = StringIO()\n",
    "export_graphviz(classifier4, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = classifier4.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = [p[1] for p in prob_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print( roc_auc_score(y, prob_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y, prob_y)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.0,1.0])\n",
    "plt.ylim([-0.0,1.0])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There has to be a better way\n",
    "\n",
    "We will finally try to approach this problem with what we call an ensemble, that is a combination of machine learning algorithms. In this case we will use a Random Forest. As we discussed, a random forest is a combination of randomly generated decision trees trained with a random sample of the data. This tends to produce way better results than a single decision tree, in particular when we have unbalanced classes (like this example). Let's see:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the original data, without upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Separate input features (X) and target variable (y)\n",
    "y = tree_dataset.In_hospital_death\n",
    "X = tree_dataset.drop('In_hospital_death', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_5 = RandomForestClassifier(max_depth=8)\n",
    "clf_5.fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_5 = clf_5.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_5 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(pred_y_5)\n",
    "predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How's our accuracy?\n",
    "print( accuracy_score(y, pred_y_5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about AUROC?\n",
    "prob_y_5 = clf_5.predict_proba(X)\n",
    "\n",
    "prob_y_5 = [p[1] for p in prob_y_5]\n",
    "\n",
    "print( roc_auc_score(y, prob_y_5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y, prob_y_5)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.0,1.0])\n",
    "plt.ylim([-0.0,1.0])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
